{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - TF-IDF Classifier\n",
    "\n",
    "Ваша цель обучить классификатор который будет находить \"токсичные\" комментарии и опубликовать решения на Kaggle [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)\n",
    "\n",
    "В процессе обучения нужно ответить на ***[вопросы](https://docs.google.com/forms/d/e/1FAIpQLSd9mQx8EFpSH6FhCy1M_FmISzy3lhgyyqV3TN0pmtop7slmTA/viewform?usp=sf_link)***\n",
    "\n",
    "Данные можно скачать тут - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('./input/train.csv').fillna('Unknown')\n",
    "test = pd.read_csv('./input/test.csv').fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стадартными подходами для анализа текста являются [Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) и его модификация [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Они реалзованны в `sklearn` в виде [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) и [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "Более подробней про них можно посмотреть [тут](https://github.com/udsclub/workshop/blob/master/notebooks/UDS-workshop-feature-extraction-and-engineering.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "re_tok = re.compile('([%s“”¨«»®´·º½¾¿¡§£₤‘’])' % string.punctuation)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', clean_text(s)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуйте разные Vectorizer и разные размеры n-gramm, стоп-слова, обрезку редких слов, обрезку слишком частых слов\n",
    "word_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                  ngram_range=(1, 2),\n",
    "                                  tokenizer=tokenize,\n",
    "                                  stop_words='english',\n",
    "                                  max_df=0.9,\n",
    "                                  min_df=3,\n",
    "                                  strip_accents='unicode', \n",
    "                                  use_idf=True,\n",
    "                                  smooth_idf=True, \n",
    "                                  sublinear_tf=True,\n",
    "                                  max_features=300000)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  smooth_idf=True,\n",
    "                                  tokenizer=tokenize,\n",
    "                                  strip_accents='unicode',\n",
    "                                  analyzer='char',\n",
    "                                  max_df=0.9,\n",
    "                                  min_df=3,\n",
    "                                  ngram_range=(1, 4),\n",
    "                                  max_features=300000)\n",
    "\n",
    "#vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=2)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_features = sparse.hstack([train_char_features, train_word_features])\n",
    "test_word_features = sparse.hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump([train_word_features, test_word_features], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    train_word_features, test_word_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опубликуйте лучшие решение на [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=3.15, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, \n",
    "                                       dual=self.dual,\n",
    "                                       class_weight='balanced',\n",
    "                                       solver='newton-cg', \n",
    "                                       max_iter=1000,\n",
    "                                       tol=0.0001,\n",
    "                                       n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "train_submission = pd.DataFrame.from_dict({'id': train['id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X_train, X_test, y_train, y_test, i):\n",
    "    np.random.seed(i)\n",
    "    ids = np.random.choice(np.arange(len(y_train), dtype=np.int32), \n",
    "                           size=int(len(y_train) * 0.8))\n",
    "    x = X_train[ids]\n",
    "    y = y_train[ids]\n",
    "    classifier = NbSvmClassifier(C=0.45)\n",
    "    classifier.fit(x, y)\n",
    "    train_score = eval_roc(classifier, X_train, y_train)\n",
    "    val_score = eval_roc(classifier, X_test, y_test)\n",
    "    train_proba = classifier.predict_proba(train_word_features)[:, 1]\n",
    "    proba = classifier.predict_proba(test_word_features)[:, 1]\n",
    "    return train_score, val_score, train_proba, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "scores = []\n",
    "for class_name in class_names:\n",
    "    print('Class: %s' % class_name)\n",
    "    probas = []\n",
    "    train_probas = []\n",
    "    train_target = np.array(train[class_name])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_word_features, \n",
    "                                                        train_target, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=0xCAFFE)\n",
    "                                                        #stratify=train_target)\n",
    "    \n",
    "    train_score, val_score = [], []\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=predictors) as executor:\n",
    "        futures = (executor.submit(training, X_train, X_test, y_train, y_test, i) for i in range(predictors))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            t_score, v_score, train_proba, proba = future.result()\n",
    "            train_score.append(t_score)\n",
    "            val_score.append(v_score)\n",
    "            train_probas.append(train_proba)\n",
    "            probas.append(proba)\n",
    "    \n",
    "    scores.append(np.mean(val_score))\n",
    "    print('\\tTrain ROC-AUC: %s' % np.mean(train_score))\n",
    "    print('\\tVal ROC-AUC: %s' % np.mean(val_score))\n",
    "    submission[class_name] = np.mean(probas, axis=0)\n",
    "    train_submission[class_name] = np.mean(train_probas, axis=0)\n",
    "print('Total: %s' % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_nb_logistic_regression_010.csv', index=False)\n",
    "train_submission.to_csv('train_nb_logistic_regression_010.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
